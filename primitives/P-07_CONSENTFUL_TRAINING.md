# P-07 — Consentful Training  
**Primitive Type:** Knowledge Governance  
**Status:** Canonical  
**Version:** v0.1  

---

## Purpose  
Prevent silent extraction of human knowledge into AI training systems without explicit permission.

Consentful Training formalizes a simple rule:

**AI may learn from human knowledge only under explicit permission and traceable attribution.**

This is not anti-AI.

This is pro-civilization.

---

## Why This Exists  
Most current AI deployment is operating inside a legal and ethical fog:

- institutions fear liability  
- authors fear theft  
- students fear exposure  
- universities write comfort clauses like:  
  **“Your data will not be used to train.”**

But that clause is not a long-term solution.

It is a temporary risk shield.

The actual sustainable policy is this:

**Training must be consentful, auditable, and attributable.**

---

## The Canonical Rule  
If a human did not explicitly consent to training,  
**the model must not ingest it.**

If the model did ingest it,  
**the system must be able to prove how, when, and under what license.**

---

## Operator Checklist (30 Seconds)  
Before allowing any dataset, corpus, logs, or student work to be used for training:

- **Is consent explicit?**  
- **Is attribution traceable?**  
- **Is licensing unambiguous?**  
- **Is revocation possible?**  
- **Is usage auditable?**  
- **Is the training boundary enforceable?**

If any answer is unclear:

**Training is not governance-safe.**

---

## The Consent Gradient (Simple Model)  

### Level 0 — No Consent  
Data was collected incidentally or silently.

**Training forbidden.**

---

### Level 1 — Passive Consent  
Consent is implied by platform usage.

**Training fragile and legally unstable.**

---

### Level 2 — Explicit Consent  
User opts in knowingly.

**Training permitted, but must remain auditable.**

---

### Level 3 — Explicit Consent + Attribution  
User opts in and is credited.

**Training becomes amplification rather than extraction.**

---

### Level 4 — Consent + Attribution + Compensation  
User opts in, is credited, and receives value.

**Training becomes partnership infrastructure.**

---

## What This Prevents  
Consentful Training blocks four failure modes:

1. **Silent exploitation**  
2. **Institutional liability laundering**  
3. **Author distrust / backlash cascades**  
4. **Educational collapse through epistemic theft perception**

---

## Must-Use Companion Primitives  
Consentful Training should always be paired with:

- **P-05 Coherence Audit Stamp**  
  (assumptions + uncertainty + rollback + responsibility)

- **P-06 Rollback Covenant**  
  (revocation must exist)

- **P-04 Responsibility Handshake**  
  (someone must own the consequence of training ingestion)

---

## Default Refusal Behavior  
If consent is ambiguous, missing, or undocumented:

**REFUSE TRAINING.**  
**Log the refusal.**  
**Request explicit consent and licensing terms.**

---

## Operator One-Liner (Canonical)  
**If training is not consentful, AI progress becomes theft disguised as innovation.**

---

## Field Note  
Universities should not fear AI training.

They should demand that training becomes:

- permissioned  
- attributable  
- revocable  
- legally legible  

Because that is how knowledge becomes safely amplifiable.

This is not a restriction.

It is the birth of a new literacy:

**How to become legible to AI without being exploited by AI.**  

---

